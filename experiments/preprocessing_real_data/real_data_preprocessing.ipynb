{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1679ce7",
   "metadata": {},
   "source": [
    "# Preprocessing of real data\n",
    "\n",
    "This notebook allows to create hourly data with precipitation, streamflow, potential evapotranspiration and date from the following files:\n",
    "\n",
    "- MeteoSwiss combiprecip product\n",
    "- Hourly streamflow data\n",
    "- Daily minimum, maximal and mean temperature to compute the daily potential evapotranspiration, which is then distributed evenly over 24 hours and saved at an hourly resolution.\n",
    "- Daily precipitation and streamflow data is also used to fill some possible gaps in the hourly ones.\n",
    "\n",
    "/!\\ The Pyeto Python package should be downloaded from [here](https://github.com/woodcrafty/PyETo) if it is not already present in the root folder of your repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a11069",
   "metadata": {},
   "source": [
    "#### START by dowloading the file 'data.zip' using the link below and extract it. Save the resulting 'data' folder in the folder 'experiments'.\n",
    "\n",
    "https://www.dropbox.com/scl/fi/mo4xg95ktj7yt09e1o78a/data.zip?rlkey=9m8e4nshv831g7q129c1vsy53&st=rgy8wp05&dl=0\n",
    "\n",
    "This file contains the hydrologic and climatic timeseries for a single demonstration catchment (Minster river in Euthal/RÃ¼ti (canton Schwyz), BAFU data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6550c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "from helpers_preprocessing import get_acronym, get_latitude, get_PET_hargreaves, process_precip, process_hydro, merging_dfs\n",
    "\n",
    "ALL_GIS_IDs = ['44']  # Catchments of interest's GIS IDs (only one catchment here with ID #44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66555a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths with catchment(s) properties and fluxes\n",
    "path_J = os.path.join('..', 'data', 'real_data', 'polygons_CH1903_LV95_area_weighted_combiprecip')  # fluxes for only one catchment right now\n",
    "path_daily = os.path.join('..', 'data', 'real_data', 'Daily_Data')\n",
    "path_Q  = os.path.join('..', 'data', 'real_data', 'hourly_streamflow')\n",
    "file_catchprop = os.path.join('..', 'data', 'CH_Catchments_Geodata_MF_20221209.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cae2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catchment static properties\n",
    "df_catchment_properties = pd.read_csv(file_catchprop, header=None,  engine='python')  # why full of NaNs? RM\n",
    "\n",
    "# Save catchment properties headers\n",
    "data_type = df_catchment_properties.iloc[0, :]  # category\n",
    "description = df_catchment_properties.iloc[1, :]  # feature\n",
    "IDs = df_catchment_properties.iloc[2, :]  # feature ID/label\n",
    "\n",
    "df_catchment_properties = df_catchment_properties.drop([0, 1, 2])  # remove headers\n",
    "rows_idx = list(df_catchment_properties.iloc[:, 0])\n",
    "\n",
    "df_info = df_catchment_properties.iloc[:, [0, 9]]  # collect catchment ID and information on data quality\n",
    "df_info.columns = ['GIS_ID', 'INFO']\n",
    "\n",
    "# Translate categories from german to english and associate\n",
    "# each category with its corresponding features\n",
    "categories = ['Area', 'Quality', 'Response', 'Climate', 'Altitude', 'Slope', 'runoff accumulation', \n",
    "              'storage capacity', 'permeability', 'waterlogging', 'thoroughness', 'land use',\n",
    "              'ground cover' ] + 5*['Geology'] + ['Quaternary Deposits']\n",
    "category = None\n",
    "category2feature = {'Area': []}\n",
    "\n",
    "count = 0\n",
    "for i in range(len(data_type)):\n",
    "    if type(data_type[i]) != str:\n",
    "        category2feature[categories[count]].append(IDs[i])\n",
    "    else:\n",
    "        count += 1\n",
    "        try:\n",
    "            category2feature[categories[count]].append(IDs[i])\n",
    "        except:\n",
    "            category2feature[categories[count]] = [IDs[i]]\n",
    "\n",
    "# Associate each feature to its category (defined with an ID)\n",
    "features2idxcategory = {}\n",
    "for i, cat in enumerate(categories):\n",
    "    for feature in category2feature[cat]:\n",
    "        features2idxcategory[feature] = i\n",
    "df_catchment_properties.columns = IDs\n",
    "\n",
    "df_catchment_names = df_catchment_properties[list(IDs[:5]) + ['H_MIN', 'H_MAX', 'H_MEAN']]  # retrieve some specific columns, why? RM\n",
    "\n",
    "gis_id = df_catchment_properties['GIS_ID']\n",
    "df_catchment_properties.drop(columns=['GIS_ID'], inplace=True)\n",
    "df_catchment_properties.index = rows_idx\n",
    "\n",
    "# Split main catchment information and attributes\n",
    "df_infos = df_catchment_properties.iloc[:, 1:4]\n",
    "df_data = df_catchment_properties.iloc[:, 4:]\n",
    "df_data = df_catchment_properties.drop(df_data.columns[[3, 4]], axis = 1)\n",
    "\n",
    "# Generate a unique ID for each catchment based on data source and numeric ID\n",
    "betreiber2acronym = {'BAFU':'BAFU-',\n",
    "                     'AARGAU':'AG',\n",
    "                     'SOLOTHURN':'SO-',\n",
    "                     'BERN':'BE-',\n",
    "                     'BASEL LANDSCHAFT': 'BL-',\n",
    "                     'LUZERN':'LU-',\n",
    "                     'ZUERICH':'ZH-'}\n",
    "\n",
    "df_catchment_names = df_catchment_names.dropna(subset=['org_ID'])\n",
    "df_catchment_names['catchment_name'] = (\n",
    "    get_acronym(df_catchment_names['Betreiber'].astype(str), betreiber2acronym)\n",
    "    + df_catchment_names[\"org_ID\"]\n",
    ")\n",
    "df_catchment_names.index = range(len(df_catchment_names['catchment_name']))\n",
    "df_catchment_names['catchment_name'] = df_catchment_names['catchment_name'].apply(lambda x: x.replace('_','-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a28f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep main catchment information for catchment with predefined ID only\n",
    "df_catchment_names = df_catchment_names[df_catchment_names['GIS_ID'].apply(lambda x : (x in ALL_GIS_IDs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b8bae",
   "metadata": {},
   "source": [
    "# PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9856c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent='myapplication')\n",
    "\n",
    "# Retrieve latitude from river name (why?)\n",
    "latitudes = list(map(lambda x: get_latitude(geolocator, x), df_catchment_names['Gewaesser']))\n",
    "\n",
    "dic_latitudes = {}\n",
    "for i in range(len(df_catchment_names['Gewaesser'])):\n",
    "    dic_latitudes[df_catchment_names['GIS_ID'].iloc[i]] = latitudes[i]\n",
    "\n",
    "# Save dictionnary relating catchment ID with river latitude\n",
    "with open('../data/real_data/dic_latitudes.pkl', 'wb') as handle:\n",
    "    pickle.dump(dic_latitudes, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1033af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments:  1\n",
      "Add Latitudes\n",
      "Number of catchments:  1\n",
      "Remove catchments with data issues\n",
      "Number of catchments:  1\n"
     ]
    }
   ],
   "source": [
    "# Add latitude to catchment ID\n",
    "df_catchment_names['latitude'] = [dic_latitudes[GISID] for GISID in df_catchment_names['GIS_ID']]\n",
    "print('Number of catchments: ', df_catchment_names.shape[0])\n",
    "print('Add Latitudes')\n",
    "df_catchment_names = df_catchment_names.drop(df_catchment_names[df_catchment_names['latitude'] == np.nan].index)\n",
    "print('Number of catchments: ', df_catchment_names.shape[0])\n",
    "\n",
    "# Filter catchment with data quality issues\n",
    "locations_pb = df_info.dropna(subset=['INFO'])['GIS_ID'].to_numpy()  # select stations with a non NaN value in 'INFO', meaning there might be a quality problem\n",
    "print('Remove catchments with data issues')\n",
    "df_catchment_names = df_catchment_names[df_catchment_names['GIS_ID'].apply(lambda x: not(x in locations_pb))]\n",
    "print('Number of catchments: ', df_catchment_names.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a2e15",
   "metadata": {},
   "source": [
    "# Processing all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688b33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:05,  5.33s/it]\n"
     ]
    }
   ],
   "source": [
    "# Prepare a data file for each station/catchment of interest with all timeseries necessary to run GAMCR\n",
    "for index, row in tqdm(df_catchment_names.iterrows()):\n",
    "    GISID = row['GIS_ID']\n",
    "\n",
    "    # Discharge data\n",
    "    discharge = process_hydro(GISID, path_Q, df_catchment_names, path_daily)\n",
    "\n",
    "    # Precipitation data\n",
    "    precip = process_precip(GISID, path_J, path_daily)\n",
    "\n",
    "    # Merge discharge and precipitation\n",
    "    data = merging_dfs(precip, discharge)\n",
    "\n",
    "    # Temperature\n",
    "    Tmax = pd.read_csv(os.path.join(path_daily, 'Tmax', 'TmaxD_GIS_ID-{0}.csv').format(GISID), header=0,  engine='python')[['mean','datetime']]\n",
    "    Tmax['datetime'] = Tmax['datetime'].apply(lambda x: datetime.strptime(str(x), '%Y-%m-%d'))\n",
    "    Tmax = Tmax.rename(columns={'mean': 'tmax'})\n",
    "    Tmax.sort_values(by=['datetime'])\n",
    "\n",
    "    Tmin = pd.read_csv(os.path.join(path_daily, 'Tmin', 'TminD_GIS_ID-{0}.csv').format(GISID), header=0,  engine='python')[['mean','datetime']]\n",
    "    Tmin['datetime'] = Tmin['datetime'].apply(lambda x: datetime.strptime(str(x), '%Y-%m-%d'))\n",
    "    Tmin.sort_values(by=['datetime'])\n",
    "    Tmin = Tmin.rename(columns={'mean': 'tmin'})\n",
    "\n",
    "    Tabs = pd.read_csv(os.path.join(path_daily, 'Tabs', 'TabsD_GIS_ID-{0}.csv').format(GISID), header=0,  engine='python')[['mean','datetime']]\n",
    "    Tabs['datetime'] = Tabs['datetime'].apply(lambda x: datetime.strptime(str(x), '%Y-%m-%d'))\n",
    "    Tabs.sort_values(by=['datetime'])\n",
    "    Tabs = Tabs.rename(columns={'mean': 'tabs'})\n",
    "\n",
    "    data_daily = Tmin\n",
    "    data_daily = pd.merge(data_daily, Tmax, on=\"datetime\", how=\"inner\")\n",
    "    data_daily = pd.merge(data_daily, Tabs, on=\"datetime\", how=\"inner\")\n",
    "\n",
    "    # PET\n",
    "    lst = np.arange(0,len(data_daily),1)\n",
    "    PET = np.array(list(map(lambda t: get_PET_hargreaves(data_daily.iloc[t]['tmin'], data_daily.iloc[t]['tabs'], data_daily.iloc[t]['tmax'], data_daily['datetime'][t], dic_latitudes[GISID]), lst) ))\n",
    "    data_daily['pet'] = PET\n",
    "    data_daily.set_index('datetime', inplace=True)\n",
    "    data_hourly = data_daily.resample('h').ffill()\n",
    "    df_treated_data = pd.merge(data, data_hourly, on=\"datetime\", how=\"inner\")\n",
    "\n",
    "    # Export processed data\n",
    "    path_data_export = os.path.join('..', 'data', 'real_data', 'GISID2hourly_data_withPET', f'{GISID}.csv')\n",
    "    df_treated_data.to_csv(path_data_export, index=False, header=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gamcr-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
